%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ELEC-E8740 Sensor Fusion Project Report
% Authors: Haoran Cao, Xingle Zhang
% Using fphw template
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
	11pt, % Default font size
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathpazo}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{multicol,multirow}
\usepackage{diagbox}
\usepackage[flushleft]{threeparttable}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Final Group Project Report} % Assignment title
\author{Haoran Cao, Xingle Zhang} % Student names
\date{December 12, 2024} % Due date
\institute{Aalto University} % Institute or school name
\class{ELEC-E8740 - Basics of Sensor Fusion D} % Course or class name
\professor{Simo Särkkä} % Professor or teacher in charge

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
\noindent This report presents the implementation and results of a sensor fusion system for tracking a DiddyBorg autonomous robot. The project consists of two main parts: sensor calibration (Part I) and localization/tracking algorithms (Part II). In Part I, we calibrated the Inertial Measurement Unit (IMU), camera module, and motor controller to establish accurate sensor models. In Part II, we developed three tracking approaches: camera-based static localization using QR code detection, dead reckoning with IMU and motor data, and sensor fusion through both Particle Filter and Extended Kalman Filter (EKF). We implemented and compared two nonlinear filtering approaches, with the optimized EKF achieving superior performance—only 4.9 cm drift over three complete laps, significantly outperforming dead reckoning (21.4° heading error) and the Particle Filter (11.4 cm drift). The success of the EKF demonstrates that proper parameter tuning and outlier rejection are often more critical than algorithmic complexity.
\end{abstract}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

Mobile robot localization is a fundamental challenge in autonomous navigation systems. The ability to accurately estimate a robot's position and orientation in real-time is essential for path planning, obstacle avoidance, and task execution. This project addresses the localization problem using a multi-sensor fusion approach that combines inertial measurements, visual landmarks, and odometry data.

\subsection{Project Overview}

The objective of this project is to develop and validate a sensor fusion system for tracking a DiddyBorg rover-type robot operating in a controlled 121.5 cm × 121.5 cm environment. The robot is equipped with:
\begin{itemize}
    \item An Inertial Measurement Unit (IMU) providing accelerometer, gyroscope, and magnetometer readings
    \item A camera module for detecting QR code landmarks with known global positions
    \item Motor encoders for odometry measurements
    \item An infrared (IR) line sensor for path following
\end{itemize}

The robot is programmed to follow a black line on a white surface, completing a semi-elliptical trajectory while we estimate its position using various sensor fusion techniques.

\subsection{Project Structure}

The project is divided into two main parts:

\textbf{Part I: Sensor Modeling and Calibration} (Tasks 1-4) involves:
\begin{enumerate}
    \item Static IMU analysis to determine gyroscope bias and variance
    \item Accelerometer calibration to estimate gain and bias parameters
    \item Camera module calibration using pinhole projection model
    \item Motor speed characterization at 30\% PWM input
\end{enumerate}

\textbf{Part II: Localization and Tracking} (Tasks 5-7) includes:
\begin{enumerate}
    \item Static localization using camera-based QR code detection
    \item Dead reckoning trajectory estimation using IMU and motor data
    \item Sensor fusion tracking using Extended Kalman Filter (EKF) and Particle Filter
\end{enumerate}

\subsection{Coordinate Systems}

Understanding the coordinate system transformations is crucial for this project. Three coordinate systems are involved:

\begin{itemize}
    \item \textbf{Global/Inertial Frame}: Fixed reference frame with origin at the bottom-left corner of the arena
    \item \textbf{Robot/Camera Frame}: Right-hand coordinate system (x-forward, y-left, z-up)
    \item \textbf{IMU Frame}: Different orientation (-x forward, -y right, z-up) requiring coordinate transformations
\end{itemize}

The camera coordinate system is parallel to the robot body frame, simplifying geometric calculations for vision-based measurements.

%----------------------------------------------------------------------------------------
%	PART I: SENSOR MODELING
%----------------------------------------------------------------------------------------

\section{Part I: Sensor Modeling and Calibration}

This section describes the calibration procedures and results for each sensor modality. Accurate sensor models are essential for achieving reliable state estimation in Part II.

%----------------------------------------------------------------------------------------
\subsection{Task 1: Static IMU Experiment}

\subsubsection{Objective}
Determine the bias and variance of the gyroscope measurements when the robot is stationary. This establishes the baseline noise characteristics of the IMU.

\subsubsection{Methodology}
The robot was placed on a stable, horizontal surface for approximately 45 seconds while collecting IMU data at 20 Hz sampling rate. For each gyroscope axis (x, y, z), we computed:
\begin{align}
    \text{Bias}_i &= \frac{1}{N}\sum_{k=1}^{N} \omega_i(k), \quad i \in \{x, y, z\} \\
    \text{Variance}_i &= \frac{1}{N-1}\sum_{k=1}^{N} (\omega_i(k) - \text{Bias}_i)^2
\end{align}
where $\omega_i(k)$ represents the gyroscope reading for axis $i$ at time step $k$, and $N$ is the total number of samples.

\subsubsection{Results}

The static IMU experiment yielded the following gyroscope parameters:

\begin{table}[H]
\centering
\caption{Gyroscope Bias and Variance (Task 1)}
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{X-axis} & \textbf{Y-axis} & \textbf{Z-axis} \\
\midrule
Bias (deg/s) & $-0.0324$ & $-0.2039$ & $0.1873$ \\
Variance ((deg/s)$^2$) & $0.0414$ & $0.0477$ & $0.0756$ \\
Std. Dev. (deg/s) & $0.2034$ & $0.2184$ & $0.2750$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task1_gyroscope_analysis_converted.png}
\caption{Task 1: Static IMU measurements showing gyroscope readings over time (top left), angular velocity distribution (top right), accelerometer readings (bottom left), and magnetometer readings (bottom right). The gyroscope exhibits zero-mean noise centered around the bias values, while the accelerometer correctly measures gravity ($\approx 1g$) in the z-axis.}
\label{fig:task1}
\end{figure}

\subsubsection{Discussion}

From the static IMU measurements, we observe that the accelerometer correctly reads approximately 1g in the z-axis (confirming the sensor is measuring gravity), while the x and y axes read near zero as expected for a level surface. The gyroscope shows small non-zero mean values representing bias, particularly notable in the y-axis (-0.2039 deg/s) and z-axis (0.1873 deg/s).

The results reveal several important characteristics:
\begin{itemize}
    \item The gyroscope bias is relatively small (< 0.21 deg/s) but non-negligible, particularly for the y and z axes
    \item The z-axis exhibits the highest variance, which is typical for MEMS gyroscopes
    \item The accelerometer correctly measures gravitational acceleration ($\approx 1.0165g$) in the z-axis, confirming proper sensor orientation
    \item The magnetometer readings are stable, showing minimal drift
\end{itemize}

These bias values will be subtracted from all subsequent gyroscope measurements to improve heading estimation accuracy.

%----------------------------------------------------------------------------------------
\subsection{Task 2: Accelerometer Calibration}

\subsubsection{Objective}
Estimate the gain ($k_i$) and bias ($b_i$) parameters for each accelerometer axis using gravity as a reference.

\subsubsection{Methodology}

The accelerometer calibration exploits the fact that Earth's gravitational field provides a constant, known reference acceleration. The sensor model for each axis is:
\begin{equation}
    y_i = k_i \cdot a_i + b_i + r_i
\end{equation}
where $y_i$ is the measured acceleration, $a_i$ is the true acceleration, $k_i$ is the gain, $b_i$ is the bias, and $r_i$ is measurement noise.

For calibration, the robot was positioned with each body axis aligned with gravity (both +1g and -1g orientations), yielding six distinct orientations: +z, -z, +x, -x, +y, -y. For each axis $i$:

\begin{align}
    k_i &= \frac{a_{\text{up}} - a_{\text{down}}}{2g} \\
    b_i &= \frac{a_{\text{up}} + a_{\text{down}}}{2}
\end{align}

where $a_{\text{up}}$ and $a_{\text{down}}$ are the average readings when the axis is aligned upward and downward, respectively, and $g = 9.81$ m/s$^2$.

\subsubsection{Results}

\begin{table}[H]
\centering
\caption{Accelerometer Calibration Parameters (Task 2)}
\begin{tabular}{lccc}
\toprule
\textbf{Parameter} & \textbf{X-axis} & \textbf{Y-axis} & \textbf{Z-axis} \\
\midrule
Gain $k_i$ & $0.9975$ & $0.9929$ & $0.9961$ \\
Bias $b_i$ (g) & $0.0136$ & $-0.0042$ & $0.0193$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task2_accelerometer_calibration.png}
\caption{Task 2: Accelerometer calibration showing raw readings across six orientations (top left), distribution of measurements (top right), 2D scatter plot of x-y acceleration colored by z-axis (bottom left), and acceleration magnitude verification (bottom right). The magnitude plot confirms that after calibration, $\|\bm{a}\| \approx 1g$ for all static orientations.}
\label{fig:task2}
\end{figure}

\subsubsection{Discussion}

The calibration results demonstrate:
\begin{itemize}
    \item All gain factors are very close to unity (0.993--0.998), indicating minimal scale error
    \item Bias values are small (< 0.02g), suggesting good factory calibration
    \item The acceleration magnitude plot shows that $\sqrt{a_x^2 + a_y^2 + a_z^2} \approx 0.9998g$ across all orientations, validating the calibration accuracy
\end{itemize}

These parameters will be applied to all subsequent accelerometer measurements to correct for systematic errors.

%----------------------------------------------------------------------------------------
\subsection{Task 3: Camera Module Calibration}

\subsubsection{Objective}
Determine the focal length ($f$) and distance bias ($b$) of the camera module using the pinhole projection model.

\subsubsection{Methodology}

The pinhole camera model relates the detected height of a QR code in the image plane to its distance from the camera:
\begin{equation}
    h = \frac{h_0 \cdot f}{d + b}
\end{equation}
where:
\begin{itemize}
    \item $h$ = detected height in pixels
    \item $h_0$ = actual QR code height (11.5 cm)
    \item $f$ = focal length in pixels
    \item $d$ = measured distance from camera to wall
    \item $b$ = bias accounting for camera offset and wall thickness
\end{itemize}

Rearranging this equation:
\begin{equation}
    d = \frac{h_0 \cdot f}{h} - b
\end{equation}

Taking the reciprocal:
\begin{equation}
    \frac{1}{h} = \frac{1}{h_0 \cdot f} \cdot d + \frac{b}{h_0 \cdot f}
\end{equation}

This yields a linear relationship between $1/h$ and $d$, where:
\begin{itemize}
    \item Slope = $\frac{1}{h_0 \cdot f}$, so $f = \frac{1}{\text{slope} \cdot h_0}$
    \item Intercept = $\frac{b}{h_0 \cdot f}$, so $b = \text{intercept} \cdot h_0 \cdot f$
\end{itemize}

A single QR code was placed at varying distances (34.2 cm to 141.2 cm) and the detected height was recorded. Linear least-squares regression was applied to the $(d, 1/h)$ data.

Additionally, the bias includes:
\begin{itemize}
    \item Camera pinhole to IR detector offset: 1.7 cm
    \item Wall to wooden frame distance: 5 cm
\end{itemize}

\subsubsection{Results}

Linear regression yielded:
\begin{itemize}
    \item Slope: $k = 6034.33$ cm$\cdot$pixels
    \item Intercept: $3.42$ cm
    \item $R^2 = 0.9997$ (excellent fit)
\end{itemize}

From these, we derive:
\begin{align}
    f &= \frac{k}{h_0} = \frac{6034.33}{11.5} = 524.72 \text{ pixels} \\
    b &= \text{intercept} = 3.42 \text{ cm}
\end{align}

\begin{table}[H]
\centering
\caption{Camera Calibration Parameters (Task 3)}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Focal length $f$ & 524.72 pixels \\
Distance bias $b$ & 3.42 cm \\
$R^2$ coefficient & 0.9997 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task3_camera_calibration.png}
\caption{Task 3: Camera calibration showing linear regression of $1/h$ vs. distance (left) and the calibrated distance model (right). The excellent $R^2 = 0.9997$ indicates that the pinhole model accurately describes the camera geometry.}
\label{fig:task3}
\end{figure}

\subsubsection{Discussion}

The camera calibration demonstrates:
\begin{itemize}
    \item The pinhole model provides an excellent fit to the experimental data ($R^2 \approx 1.0$)
    \item The focal length of 524.72 pixels is consistent with typical webcam specifications
    \item The 3.42 cm bias accounts for physical offsets in the camera mounting
    \item The model is valid over a wide range (34--141 cm), covering the entire arena
\end{itemize}

These parameters enable accurate distance and bearing measurements to QR codes for localization.

%----------------------------------------------------------------------------------------
\subsection{Task 4: Motor Speed Calibration}

\subsubsection{Objective}
Characterize the robot's linear velocity as a function of Pulse Width Modulation (PWM) input to the motors.

\subsubsection{Methodology}

The robot was commanded to move forward at 30\% PWM while its position was manually measured at 40 cm intervals using a tape measure. The time to traverse each segment was recorded. Speed was calculated as:
\begin{equation}
    v = \frac{\Delta d}{\Delta t}
\end{equation}

where $\Delta d = 40$ cm is the segment length and $\Delta t$ is the measured time interval.

\subsubsection{Results}

\begin{table}[H]
\centering
\caption{Motor Speed Calibration Results (Task 4)}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Average speed at 30\% PWM & 6.23 cm/s \\
Estimated full speed (100\% PWM) & 20.77 cm/s \\
Standard deviation & 0.34 cm/s \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task4_motor_speed.png}
\caption{Task 4: Motor speed calibration showing cumulative distance vs. time (left) and speed per segment (right). The robot exhibits relatively consistent velocity after an initial acceleration phase, with mean speed of 6.23 cm/s at 30\% PWM.}
\label{fig:task4}
\end{figure}

\subsubsection{Discussion}

Key observations:
\begin{itemize}
    \item The first segment (0--40 cm) shows lower speed (4.43 cm/s), indicating an acceleration phase
    \item After the initial segment, speed stabilizes around 6--7 cm/s
    \item Speed variation (std = 0.34 cm/s) is relatively small, suggesting good motor control
    \item Assuming linear PWM-to-velocity relationship, full-speed estimate is $\approx 20.77$ cm/s
\end{itemize}

This calibration provides the velocity input for the dead reckoning algorithm in Part II.

%----------------------------------------------------------------------------------------
%	PART II: LOCALIZATION AND TRACKING
%----------------------------------------------------------------------------------------

\section{Part II: Localization and Tracking}

Building on the sensor models from Part I, this section implements three localization algorithms with increasing complexity: static camera-based localization, dead reckoning, and sensor fusion via both Extended Kalman Filter and Particle Filter.

%----------------------------------------------------------------------------------------
\subsection{Task 5: Static Localization Using Camera}

\subsubsection{Objective}
Estimate the robot's position $(p_x, p_y)$ and heading $\psi$ in the global frame using only camera measurements of multiple QR codes with known positions.

\subsubsection{Methodology}

\paragraph{Measurement Model}
For each detected QR code $i$ at known global position $(s_{x,i}, s_{y,i})$, the camera provides:
\begin{itemize}
    \item Detected height $h_i$ (pixels)
    \item Center x-coordinate $C_{x,i}$ (pixels, relative to image center)
\end{itemize}

The nonlinear measurement model is:
\begin{align}
    h_i &= \frac{h_0 \cdot f}{d_i} \\
    C_{x,i} &= f \cdot \tan(\phi_i)
\end{align}

where:
\begin{align}
    d_i &= \sqrt{(s_{x,i} - p_x)^2 + (s_{y,i} - p_y)^2} \\
    \phi_i &= \arctan\left(\frac{s_{y,i} - p_y}{s_{x,i} - p_x}\right) - \psi
\end{align}

In vector form: $\bm{y} = g(\bm{x})$ where $\bm{x} = [p_x, p_y, \psi]^T$.

\paragraph{Minimum QR Codes Required}
To estimate the robot's position and heading (3 unknowns: $p_x$, $p_y$, $\psi$), we theoretically need a minimum of 2 QR codes. Each QR code provides 2 measurements (height $h_i$ and center position $C_{x,i}$), giving us 4 equations for 3 unknowns. This provides an over-determined system suitable for least-squares optimization. However, in practice, using more QR codes (e.g., 8--9) significantly improves accuracy and robustness by averaging out measurement noise.

\paragraph{Jacobian Derivation}
For nonlinear least squares optimization (Levenberg-Marquardt), we need the Jacobian matrix $\bm{J}$. For each QR code $i$, the measurement vector is $[h_i, C_{x,i}]^T$, giving:

\begin{align}
    \frac{\partial h_i}{\partial p_x} &= \frac{h_0 f (s_{x,i} - p_x)}{d_i^3} \\
    \frac{\partial h_i}{\partial p_y} &= \frac{h_0 f (s_{y,i} - p_y)}{d_i^3} \\
    \frac{\partial h_i}{\partial \psi} &= 0 \\
    \frac{\partial C_{x,i}}{\partial p_x} &= f \sec^2(\phi_i) \cdot \frac{-(s_{y,i} - p_y)}{d_i^2} \\
    \frac{\partial C_{x,i}}{\partial p_y} &= f \sec^2(\phi_i) \cdot \frac{s_{x,i} - p_x}{d_i^2} \\
    \frac{\partial C_{x,i}}{\partial \psi} &= -f \sec^2(\phi_i)
\end{align}

For $n$ detected QR codes, the Jacobian is a $2n \times 3$ matrix.

\paragraph{Optimization}
We solve the nonlinear weighted least squares problem:
\begin{equation}
    \hat{\bm{x}} = \arg\min_{\bm{x}} \sum_{i=1}^{n} \left[\bm{y}_i - g_i(\bm{x})\right]^T \bm{R}_i^{-1} \left[\bm{y}_i - g_i(\bm{x})\right]
\end{equation}

using the Levenberg-Marquardt algorithm with measurement covariance:
\begin{equation}
    \bm{R}_i = \begin{bmatrix} \sigma_h^2 & 0 \\ 0 & \sigma_{C_x}^2 \end{bmatrix} = \begin{bmatrix} 2.0^2 & 0 \\ 0 & 3.0^2 \end{bmatrix} \text{ pixels}^2
\end{equation}

The posterior covariance is estimated from the Jacobian at convergence:
\begin{equation}
    \bm{P} = \left(\bm{J}^T \bm{R}^{-1} \bm{J}\right)^{-1}
\end{equation}

\subsubsection{Results}

The robot was positioned at true location $(59.5, 26.0)$ cm with heading $90°$ (facing north). The camera detected 9 QR codes simultaneously.

\begin{table}[H]
\centering
\caption{Task 5: Static Localization Results}
\begin{tabular}{lccc}
\toprule
\textbf{State} & \textbf{True Value} & \textbf{Estimated} & \textbf{Error} \\
\midrule
$p_x$ (cm) & 59.50 & 62.00 & 2.50 \\
$p_y$ (cm) & 26.00 & 26.72 & 0.72 \\
$\psi$ (deg) & 90.00 & 90.00 & 0.00 \\
\midrule
Position error (cm) & -- & -- & 2.60 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Posterior Uncertainty (Standard Deviations)}
\begin{tabular}{lc}
\toprule
\textbf{State} & \textbf{Std. Dev.} \\
\midrule
$\sigma_{p_x}$ & 1.82 cm \\
$\sigma_{p_y}$ & 0.49 cm \\
$\sigma_{\psi}$ & 0.99$°$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task5_final_result.png}
\caption{Task 5: Static localization results showing estimated vs. true position (top left), measurement residuals for each QR code (top right), posterior covariance matrix (bottom left), and measurement vs. prediction scatter plot (bottom right). The algorithm achieves 2.60 cm position error.}
\label{fig:task5}
\end{figure}

\subsubsection{Discussion}

The results demonstrate:
\begin{itemize}
    \item Excellent position estimation with only 2.60 cm total error
    \item Perfect heading estimation ($0°$ error), likely due to the symmetric geometry of detected QR codes
    \item Higher uncertainty in x-direction (1.82 cm) than y-direction (0.49 cm), reflecting the QR code geometry along Wall 2
    \item The Levenberg-Marquardt algorithm converged in 13 function evaluations
    \item Measurement residuals are generally small (mean 1.59 pixels), with maximum residual of 4.22 pixels
    \item The measurement vs. prediction plot shows excellent agreement (points near ideal line)
\end{itemize}

The covariance matrix reveals:
\begin{equation}
    \bm{P} = \begin{bmatrix}
    3.31 & 0.12 & -0.31 \\
    0.12 & 0.24 & 0.00 \\
    -0.31 & 0.00 & 0.03
    \end{bmatrix}
\end{equation}

The small off-diagonal terms indicate weak correlation between position and heading estimates, which is desirable.

%----------------------------------------------------------------------------------------
\subsection{Task 6: Dead Reckoning Using IMU}

\subsubsection{Objective}
Track the robot's trajectory using only IMU (gyroscope) and motor encoder data, without camera corrections. This establishes a baseline for comparing sensor fusion performance.

\subsubsection{Methodology}

\paragraph{Motion Model}
We employ a quasi-constant turn rate model in continuous time:
\begin{align}
    \dot{p}_x(t) &= v(t) \cos(\phi(t)) \\
    \dot{p}_y(t) &= v(t) \sin(\phi(t)) \\
    \dot{\phi}(t) &= \omega_{\text{gyro}}(t) + w(t)
\end{align}

where:
\begin{itemize}
    \item $(p_x, p_y)$ = position in global frame
    \item $\phi$ = heading angle
    \item $v$ = linear velocity from motor encoders
    \item $\omega_{\text{gyro}}$ = angular velocity from gyroscope (bias-corrected)
    \item $w(t)$ = process noise
\end{itemize}

\paragraph{Discretization}
Using Euler integration (first-order forward difference method) with time step $\Delta t$, which is simple and appropriate for the $\sim$20 Hz IMU sampling rate:
\begin{align}
    p_{x,k+1} &= p_{x,k} + v_k \cos(\phi_k) \Delta t \\
    p_{y,k+1} &= p_{y,k} + v_k \sin(\phi_k) \Delta t \\
    \phi_{k+1} &= \phi_k + \omega_k \Delta t
\end{align}

\paragraph{Velocity Estimation}
From Task 4, we know that at 30\% PWM, $v = 6.23$ cm/s. For arbitrary PWM inputs:
\begin{equation}
    v_k = \frac{6.23}{0.30} \cdot \frac{\text{PWM}_{\text{left}} + \text{PWM}_{\text{right}}}{2}
\end{equation}

\paragraph{Gyroscope Processing}
Raw gyroscope measurements are corrected for bias (from Task 1):
\begin{equation}
    \omega_k = \omega_{\text{raw},k} - b_z = \omega_{\text{raw},k} - 0.1873 \text{ deg/s}
\end{equation}

and converted to radians/s.

\paragraph{Initial Conditions}
From the project specification:
\begin{itemize}
    \item Initial position: $(p_x, p_y) = (43, 18)$ cm
    \item Initial heading: $\phi_0 = 0°$ (facing east)
\end{itemize}

\subsubsection{Results}

The robot completed 3 laps of the semi-elliptical track in approximately 3.34 minutes (203 seconds).

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task6_result.png}
\caption{Task 6: Dead reckoning trajectory (left) showing the estimated path compared to the reference track, and heading angle evolution over time (right). The trajectory exhibits significant drift due to accumulated integration errors.}
\label{fig:task6a}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{task6_result_change_pwm.png}
\caption{Task 6: Alternative view of dead reckoning trajectory emphasizing the spiral drift pattern characteristic of uncorrected odometry.}
\label{fig:task6b}
\end{figure}

\subsubsection{Discussion}

Dead reckoning demonstrates classic failure modes:
\begin{itemize}
    \item \textbf{Heading drift}: The heading accumulates error over time, reaching $\approx 1081°$ instead of the expected $1080°$ (3 complete rotations)
    \item \textbf{Position drift}: The trajectory spirals inward/outward due to compounding integration errors
    \item \textbf{No absolute reference}: Without external corrections, even small biases in $\omega$ lead to unbounded error growth
\end{itemize}

The drift rate is approximately:
\begin{equation}
    \text{Heading error} = 1081° - 1080° = 21° \text{ over 203 seconds} \approx 0.10°/\text{s}
\end{equation}

This corresponds to a residual gyroscope bias of $\approx 0.10°$/s even after calibration, highlighting the limitations of pure dead reckoning.

Despite the drift, the overall trajectory shape resembles the reference track, confirming that:
\begin{itemize}
    \item The motion model is appropriate
    \item Velocity scaling from PWM is approximately correct
    \item Gyroscope bias correction is partially effective
\end{itemize}

This motivates the need for sensor fusion with absolute position measurements from the camera.

%----------------------------------------------------------------------------------------
\subsection{Task 7: Sensor Fusion Tracking - Algorithm Comparison}

\subsubsection{Objective}
Fuse IMU odometry (prediction) with camera QR code detections (measurement updates) to achieve accurate, drift-free tracking. We implemented and compared two nonlinear filtering approaches: Extended Kalman Filter (EKF) and Particle Filter (PF).

\subsubsection{Methodology}

\paragraph{State Vector}
\begin{equation}
    \bm{x}_k = \begin{bmatrix} p_x \\ p_y \\ \phi \end{bmatrix}_k
\end{equation}

\paragraph{Process Model}
The discrete-time motion model from Task 6:
\begin{equation}
    \bm{x}_{k+1} = f(\bm{x}_k, \bm{u}_k) + \bm{w}_k
\end{equation}

where:
\begin{align}
    f(\bm{x}_k, \bm{u}_k) = \begin{bmatrix}
        p_x + v_k \cos(\phi_k) \Delta t \\
        p_y + v_k \sin(\phi_k) \Delta t \\
        \phi_k + \omega_k \Delta t
    \end{bmatrix}
\end{align}

and $\bm{u}_k = [v_k, \omega_k]^T$ is the control input.

\paragraph{Measurement Model}
When QR codes are detected, the camera provides measurements following the model from Task 5:
\begin{equation}
    \bm{y}_k = g(\bm{x}_k) + \bm{v}_k
\end{equation}

For $n_k$ detected QR codes, each provides height $h_i$ and center position $C_{x,i}$:
\begin{align}
    h_i &= \frac{h_0 f}{\sqrt{(s_{x,i} - p_x)^2 + (s_{y,i} - p_y)^2}} \\
    C_{x,i} &= f \tan\left(\arctan\left(\frac{s_{y,i} - p_y}{s_{x,i} - p_x}\right) - \phi\right)
\end{align}

%----------------------------------------------------------------------------------------
\subsubsection{Extended Kalman Filter Implementation}

\paragraph{EKF Algorithm}
The EKF linearizes the nonlinear models around the current estimate using first-order Taylor expansion.

\textbf{Prediction Step:}
\begin{align}
    \hat{\bm{x}}_{k+1}^- &= f(\hat{\bm{x}}_k, \bm{u}_k) \\
    \bm{P}_{k+1}^- &= \bm{F}_k \bm{P}_k \bm{F}_k^T + \bm{Q}_k
\end{align}

where $\bm{F}_k$ is the Jacobian of the motion model:
\begin{equation}
    \bm{F}_k = \begin{bmatrix}
        1 & 0 & -v_k \sin(\phi_k) \Delta t \\
        0 & 1 & v_k \cos(\phi_k) \Delta t \\
        0 & 0 & 1
    \end{bmatrix}
\end{equation}

Process noise covariance with heading-dependent terms:
\begin{equation}
    \bm{Q}_k = \begin{bmatrix}
        \sigma_v^2 \cos^2(\phi) \Delta t^2 & \sigma_v^2 \cos(\phi)\sin(\phi) \Delta t^2 & 0 \\
        \sigma_v^2 \cos(\phi)\sin(\phi) \Delta t^2 & \sigma_v^2 \sin^2(\phi) \Delta t^2 & 0 \\
        0 & 0 & \sigma_\omega^2 \Delta t^2
    \end{bmatrix}
\end{equation}

\textbf{Update Step (when camera measurements available):}
\begin{align}
    \bm{K}_k &= \bm{P}_k^- \bm{H}_k^T (\bm{H}_k \bm{P}_k^- \bm{H}_k^T + \bm{R}_k)^{-1} \\
    \hat{\bm{x}}_k &= \hat{\bm{x}}_k^- + \bm{K}_k (\bm{y}_k - g(\hat{\bm{x}}_k^-)) \\
    \bm{P}_k &= (\bm{I} - \bm{K}_k \bm{H}_k) \bm{P}_k^-
\end{align}

where $\bm{H}_k$ is the Jacobian of the measurement model (derived in Task 5).

\paragraph{Outlier Rejection}
To improve robustness, we implemented chi-square gating to reject anomalous measurements:
\begin{equation}
    d^2 = (\bm{y}_k - g(\hat{\bm{x}}_k^-))^T (\bm{H}_k \bm{P}_k^- \bm{H}_k^T + \bm{R}_k)^{-1} (\bm{y}_k - g(\hat{\bm{x}}_k^-))
\end{equation}

Measurements with $d^2 > \gamma$ (threshold = 15.0) are rejected as outliers. This Mahalanobis distance-based gating proved critical for achieving high accuracy.

\paragraph{Parameter Tuning}
After systematic tuning based on physical reasoning and empirical testing, optimal EKF parameters were:
\begin{itemize}
    \item Process noise: $\sigma_v = 0.4$ cm/s (representing PWM variations and wheel slip), $\sigma_\omega = 0.04$ rad/s ($\approx 2.3°$/s)
    \item Measurement noise: $\sigma_h = 2.5$ cm (height uncertainty), $\sigma_{C_x} = 3.5°$ (angular uncertainty, converted to pixels)
    \item Outlier threshold: $\gamma = 15.0$ (chi-square, slightly above 99.9\% confidence for 2 DOF)
    \item Velocity model: Nonlinear scaling with exponent 1.15 to account for PWM nonlinearity
\end{itemize}

%----------------------------------------------------------------------------------------
\subsubsection{Particle Filter Implementation}

For comparison, we also implemented a Particle Filter with $N = 2000$ particles following the standard bootstrap filter methodology.

\paragraph{PF Algorithm}
The Particle Filter represents the posterior distribution using weighted samples without linearization.

\textbf{Prediction:} Each particle propagates according to:
\begin{equation}
    \bm{x}_{k+1}^{(i)} = f(\bm{x}_k^{(i)}, \bm{u}_k) + \bm{w}_k^{(i)}, \quad \bm{w}_k^{(i)} \sim \mathcal{N}(\bm{0}, \bm{Q}_k)
\end{equation}

\textbf{Update:} Particle weights are updated based on measurement likelihood:
\begin{equation}
    w_k^{(i)} \propto w_{k-1}^{(i)} \cdot p(\bm{y}_k | \bm{x}_k^{(i)})
\end{equation}

where the likelihood is computed using a Gaussian measurement model.

\textbf{Resampling:} Systematic resampling when effective sample size $N_{\text{eff}} = 1/\sum_{i}(w_k^{(i)})^2 < 1000$.

\paragraph{PF Parameters}
\begin{itemize}
    \item Number of particles: $N = 2000$
    \item Process noise: $\sigma_v = 2.0$ cm/s, $\sigma_\omega = 5.0°$/s
    \item Measurement noise: $\sigma_h = 2.0$ px, $\sigma_{C_x} = 3.0$ px
\end{itemize}

%----------------------------------------------------------------------------------------
\subsubsection{Results and Comparison}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{task7_ekf_optimized_result.png}
\caption{Task 7: Optimized EKF tracking results. Top left: Trajectory showing excellent alignment with reference track. Top right: Heading evolution correctly accumulating to $\approx 1075°$ for 3 laps. Bottom left: Position components showing periodic elliptical motion. Bottom middle: Position uncertainty remaining bounded below 2 cm. Bottom right: Heading uncertainty decreasing and stabilizing below 1°.}
\label{fig:task7_ekf}
\end{figure}

\begin{table}[H]
\centering
\caption{Tracking Performance Comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Dead Reckoning} & \textbf{Particle Filter} & \textbf{Optimized EKF} \\
\midrule
Final heading & 1081.4° & 1082.0° & 1075.3° \\
Heading error & $\sim 21.4°$ & $\sim 2.0°$ & $\sim 4.7°$ \\
Position drift & Unbounded & 11.4 cm & \textbf{4.9 cm} \\
Drift per lap & -- & 3.8 cm & \textbf{1.6 cm} \\
Final uncertainty $\sigma_x$ & N/A & -- & \textbf{0.77 cm} \\
Final uncertainty $\sigma_y$ & N/A & -- & \textbf{0.34 cm} \\
Measurements accepted & N/A & 1852 (100\%) & 1661 (40.0\%) \\
Outliers rejected & N/A & 0 & \textbf{2496 (60.0\%)} \\
Trajectory quality & Poor (spiral) & Good & \textbf{Excellent} \\
\midrule
\textbf{Overall Performance} & \textbf{Worst} & \textbf{Good} & \textbf{Best} \\
\bottomrule
\end{tabular}
\end{table}

%----------------------------------------------------------------------------------------
\subsubsection{Discussion}

\paragraph{EKF vs Particle Filter Performance}

The optimized EKF significantly outperformed the Particle Filter:

\textbf{Position Accuracy:}
\begin{itemize}
    \item EKF drift: 4.9 cm over 3 laps ($\approx 1.6$ cm/lap)
    \item PF drift: 11.4 cm over 3 laps ($\approx 3.8$ cm/lap)
    \item EKF achieves \textbf{57\% lower drift} than PF
\end{itemize}

\textbf{Heading Accuracy:}
\begin{itemize}
    \item Expected total rotation: $3 \times 360° = 1080°$
    \item EKF final heading: 1075.3° (error: 4.7°)
    \item PF final heading: 1082.0° (error: 2.0°)
    \item Both perform well for heading estimation
\end{itemize}

\textbf{Robustness and Outlier Handling:}
\begin{itemize}
    \item EKF with chi-square gating rejected 60\% of measurements as outliers
    \item This aggressive outlier rejection improved accuracy significantly
    \item PF accepted all measurements, making it more susceptible to noisy data
    \item The measurement noise in camera detections was higher than initially expected
\end{itemize}

\paragraph{Why EKF Outperformed PF}

Several factors contributed to the EKF's superior performance:

\begin{enumerate}
    \item \textbf{Outlier rejection}: The chi-square gating in EKF effectively identified and rejected poor camera measurements, while the PF incorporated all measurements regardless of quality.
    
    \item \textbf{Parameter optimization}: The EKF parameters ($\sigma_v = 0.4$ cm/s, $\sigma_\omega = 0.04$ rad/s) were more carefully tuned based on the actual system dynamics, representing typical odometry uncertainty rather than overly conservative values.
    
    \item \textbf{Measurement model accuracy}: The linearization in EKF was accurate enough for this application since the robot motion is relatively smooth and measurements occur frequently.
    
    \item \textbf{Computational efficiency}: EKF's analytical updates allowed for more sophisticated processing (outlier detection, adaptive tuning) within the same computational budget.
    
    \item \textbf{Particle degeneracy}: The PF with 2000 particles may still suffer from degeneracy in the high-dimensional measurement space when 8--9 QR codes are detected simultaneously, reducing effective diversity.
\end{enumerate}

\paragraph{EKF Trajectory Characteristics}

The EKF tracking demonstrates several important features:

\textbf{Trajectory Accuracy:}
\begin{itemize}
    \item The estimated trajectory closely follows the reference semi-elliptical track
    \item Minimal drift accumulation over 200+ seconds of operation
    \item Smooth transitions between straight and curved sections
    \item Final position within 5 cm of starting position (excellent loop closure)
\end{itemize}

\textbf{Uncertainty Evolution:}
\begin{itemize}
    \item Position uncertainty spikes initially (up to 2 cm) when few QR codes are visible
    \item Uncertainty drops rapidly when multiple QR codes are detected
    \item Uncertainty remains bounded throughout, never exceeding 2 cm
    \item Final uncertainties: $\sigma_x = 0.77$ cm, $\sigma_y = 0.34$ cm
    \item Heading uncertainty stabilizes below 1° after initial transient
\end{itemize}

\textbf{Heading Estimation:}
\begin{itemize}
    \item Final heading 1075.3° is very close to expected 1080° (3 complete rotations)
    \item Error of only 4.7° accumulated over 200 seconds
    \item Corresponds to residual drift rate of only 0.024°/s
    \item Camera updates effectively correct gyroscope integration drift
\end{itemize}

\paragraph{Critical Success Factors}

The successful EKF implementation required several key elements:

\begin{enumerate}
    \item \textbf{Coordinate system alignment}: Ensuring camera measurements use center-offset coordinates relative to image center, not raw pixel coordinates from corner. This was a critical debugging insight.
    
    \item \textbf{Initial state accuracy}: Initializing with accurate starting position (43, 18) cm and heading 0° was crucial for convergence.
    
    \item \textbf{Noise parameter tuning}: Finding the right balance where process noise ($\sigma_v, \sigma_\omega$) represents actual system uncertainty without being overly pessimistic.
    
    \item \textbf{Measurement noise realism}: Setting $\sigma_h = 2.5$ cm and $\sigma_{C_x} = 3.5°$ to reflect actual camera measurement quality, which was noisier than initially expected.
    
    \item \textbf{Outlier rejection threshold}: Chi-square threshold of 15.0 was critical—too low rejects good measurements, too high accepts outliers.
    
    \item \textbf{Velocity model calibration}: Nonlinear PWM-to-velocity mapping with exponent 1.15 better captured actual motor response than the linear model from Task 4.
\end{enumerate}

\paragraph{Comparison with Dead Reckoning}

The improvement over pure dead reckoning (Task 6) is dramatic:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Characteristic} & \textbf{Dead Reckoning} & \textbf{Optimized EKF} \\
\midrule
Drift behavior & Unbounded spiral & Bounded, minimal \\
Final heading error & $\sim 21°$ & $\sim 5°$ \\
Heading drift rate & 0.10°/s & 0.024°/s \\
Trajectory shape & Distorted & Accurate \\
Loop closure error & Large & 4.9 cm \\
Uncertainty & Unknown & Quantified \\
\bottomrule
\end{tabular}
\end{table}

The EKF reduced heading drift rate by 76\% compared to dead reckoning (0.024°/s vs. 0.10°/s), validating the fundamental principle of sensor fusion: combining complementary sensors (high-rate IMU with absolute camera measurements) achieves performance far exceeding either sensor alone.

\paragraph{Limitations and Future Improvements}

Despite excellent performance, some limitations remain:

\begin{itemize}
    \item \textbf{60\% outlier rate}: Indicates camera measurements are noisier than ideal—could benefit from better QR code detection algorithm or improved lighting conditions.
    
    \item \textbf{Linearization errors}: EKF assumes local linearity—could fail in scenarios with very rapid turns or large measurement gaps.
    
    \item \textbf{Heading drift}: Residual 0.024°/s drift suggests small remaining gyroscope bias or integration error.
    
    \item \textbf{Parameter sensitivity}: Performance depends on careful tuning—adaptive methods could improve robustness.
\end{itemize}

Potential improvements:

\begin{itemize}
    \item \textbf{Adaptive filtering}: Estimate $\bm{R}_k$ and $\bm{Q}_k$ online based on innovation statistics
    \item \textbf{Magnetometer fusion}: Add absolute heading reference to eliminate gyroscope drift entirely
    \item \textbf{SLAM extension}: Simultaneously estimate QR code positions for unknown environments
    \item \textbf{Iterated EKF}: Multiple linearization iterations per update for better accuracy
    \item \textbf{Unscented Kalman Filter}: Sigma-point transformation for more accurate nonlinear propagation
\end{itemize}

%----------------------------------------------------------------------------------------
%	CONCLUSION
%----------------------------------------------------------------------------------------

\section{Conclusion}

This project successfully developed and validated a complete sensor fusion system for mobile robot localization. The work progressed systematically from sensor calibration through increasingly sophisticated estimation algorithms, culminating in a highly accurate Extended Kalman Filter implementation that significantly outperformed both dead reckoning and particle filtering approaches.

\subsection{Key Achievements}

\textbf{Part I: Sensor Calibration}
\begin{itemize}
    \item Characterized IMU noise (gyroscope std. dev. 0.20--0.28 deg/s)
    \item Calibrated accelerometers with gains near unity and biases < 0.02g
    \item Determined camera focal length (524.72 pixels) with $R^2 = 0.9997$
    \item Measured robot velocity (6.23 cm/s at 30\% PWM)
\end{itemize}

\textbf{Part II: Localization Algorithms}
\begin{itemize}
    \item \textbf{Task 5}: Static localization achieved 2.60 cm position error using 9 QR codes
    \item \textbf{Task 6}: Dead reckoning demonstrated characteristic drift ($\sim 21.4°$ heading error, 0.10°/s drift rate)
    \item \textbf{Task 7}: Optimized EKF achieved outstanding performance with only 4.9 cm drift over 3 complete laps—57\% better than Particle Filter (11.4 cm drift)
\end{itemize}

\subsection{Technical Insights}

\paragraph{Sensor Modeling}
Accurate sensor models are foundational for reliable estimation. The calibration procedures revealed:
\begin{itemize}
    \item MEMS sensors exhibit small but significant biases requiring correction
    \item Simple models (linear for accelerometer, pinhole for camera) are often sufficient
    \item Characterizing noise statistics enables optimal filtering
\end{itemize}

\paragraph{Dead Reckoning Limitations}
Pure odometry inevitably drifts due to:
\begin{itemize}
    \item Integration of biased measurements
    \item Wheel slip and motor inconsistencies
    \item Discretization errors
\end{itemize}

Even with careful calibration, Task 6 showed $\sim 0.10°$/s residual heading drift—small but catastrophic over time.

\paragraph{Sensor Fusion Benefits}
The optimized EKF successfully combined complementary sensor modalities:
\begin{itemize}
    \item \textbf{IMU}: High-rate, continuous motion estimates (but drifts)
    \item \textbf{Camera}: Absolute position references (but sparse, noisy)
\end{itemize}

This fusion achieved:
\begin{itemize}
    \item Bounded uncertainty (no drift accumulation)
    \item High update rate (20 Hz predictions)
    \item Robustness to measurement outliers (60\% rejection rate)
    \item Excellent accuracy (4.9 cm drift over 200+ seconds)
    \item 76\% reduction in heading drift rate compared to dead reckoning
\end{itemize}

\paragraph{EKF vs Particle Filter}
A key finding was that the Extended Kalman Filter significantly outperformed the Particle Filter:
\begin{itemize}
    \item \textbf{EKF drift}: 4.9 cm (57\% better than PF)
    \item \textbf{PF drift}: 11.4 cm
    \item \textbf{Key advantage}: EKF's chi-square gating rejected 60\% of noisy measurements as outliers
    \item \textbf{Efficiency}: EKF's analytical updates allowed more sophisticated processing
\end{itemize}

This demonstrates that for this application, the linearization errors in EKF are negligible compared to the benefits of effective outlier rejection and parameter optimization. \textbf{The best algorithm is not always the most complex}—implementation details often matter more than algorithmic sophistication.

\paragraph{Implementation Challenges}
Critical debugging lessons learned:
\begin{enumerate}
    \item \textbf{Coordinate systems matter}: Raw pixel coordinates vs. center-offset caused major errors initially
    \item \textbf{Initial conditions are crucial}: Poor $\phi_0$ led to filter divergence
    \item \textbf{Noise tuning is critical}: Overly optimistic measurement noise caused over-reliance on bad camera data
    \item \textbf{Outlier rejection essential}: Without chi-square gating, performance degraded significantly (drift >15 cm)
    \item \textbf{Verification is essential}: Geometric checks of measurement models caught subtle bugs
\end{enumerate}

\subsection{Quantitative Performance Summary}

\begin{table}[H]
\centering
\caption{Final System Performance Metrics}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
\multicolumn{2}{c}{\textit{Static Localization (Task 5)}} \\
Position error & 2.60 cm \\
Heading error & 0.00° \\
QR codes detected & 9 \\
\midrule
\multicolumn{2}{c}{\textit{Dead Reckoning (Task 6)}} \\
Heading drift rate & $\sim 0.10°$/s \\
Total heading error & 21.4° \\
Trajectory quality & Poor (spiral) \\
\midrule
\multicolumn{2}{c}{\textit{Optimized EKF Tracking (Task 7)}} \\
Position drift (3 laps) & \textbf{4.9 cm} \\
Drift per lap & \textbf{1.6 cm} \\
Final heading error & 4.7° \\
Heading drift rate & \textbf{0.024°/s} \\
Final uncertainty $\sigma_x$ & 0.77 cm \\
Final uncertainty $\sigma_y$ & 0.34 cm \\
Measurements accepted & 40.0\% (1661/4157) \\
Outliers rejected & 60.0\% (2496/4157) \\
Trajectory quality & \textbf{Excellent} \\
\bottomrule
\end{tabular}
\end{table}

The optimized EKF reduced heading drift rate by 76\% compared to dead reckoning (0.024°/s vs. 0.10°/s) and maintained position accuracy within 5 cm over the entire 200+ second trajectory.

\subsection{Future Work}

Potential improvements for even better performance:

\begin{itemize}
    \item \textbf{Adaptive noise estimation}: Adjust $\bm{R}_k$ and $\bm{Q}_k$ online based on innovation statistics to handle varying measurement quality
    
    \item \textbf{Magnetometer fusion}: Incorporate compass measurements for absolute heading reference to eliminate gyroscope drift entirely
    
    \item \textbf{Improved camera processing}: Better QR code detection algorithms or lighting conditions to reduce the 60\% outlier rate
    
    \item \textbf{Iterated EKF}: Multiple linearization iterations per update step for improved accuracy in highly nonlinear scenarios
    
    \item \textbf{SLAM extension}: Simultaneously estimate QR code positions for operation in unknown environments
    
    \item \textbf{Multi-rate processing}: More sophisticated asynchronous sensor fusion to better handle the different sampling rates of IMU (20 Hz) and camera (5 Hz)
    
    \item \textbf{Velocity model refinement}: More accurate characterization of PWM-to-velocity mapping, potentially including turn-rate dependent corrections
\end{itemize}

\subsection{Final Remarks}

This project demonstrated that effective sensor fusion requires:
\begin{enumerate}
    \item Rigorous calibration and characterization of each sensor
    \item Understanding of each sensor's strengths and limitations
    \item Appropriate mathematical models (motion and measurement)
    \item Careful implementation and systematic debugging
    \item Empirical tuning guided by physical insight
    \item Robust outlier rejection mechanisms
\end{enumerate}

The successful optimized EKF implementation validates the theoretical foundations taught in ELEC-E8740 and provides practical experience with real-world sensor fusion challenges. The system achieves position accuracy of 4.9 cm drift over 3 complete laps—a remarkable result that demonstrates the power of proper sensor fusion.

Most importantly, the comparison between algorithms (Dead Reckoning vs. Particle Filter vs. EKF) provides valuable insights: \textbf{the best algorithm is not always the most complex}. The EKF, with proper parameter tuning and outlier rejection, outperformed the theoretically more flexible Particle Filter. This highlights that implementation details—coordinate system handling, outlier rejection, parameter optimization—often matter more than algorithmic sophistication.

The system achieves robust, accurate localization suitable for autonomous navigation tasks and provides a solid foundation for more advanced applications such as SLAM, multi-robot coordination, or outdoor navigation with GPS integration.

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\section{References}

\begin{enumerate}
    \item Course lecture materials: ELEC-E8740 Basics of Sensor Fusion, Aalto University, Fall 2024
    \item Project guide: \textit{Tracking of an Autonomous Robot}, Fatemeh Yaghoobi, October 2024
    \item Särkkä, S. (2013). \textit{Bayesian Filtering and Smoothing}. Cambridge University Press
    \item Thrun, S., Burgard, W., \& Fox, D. (2005). \textit{Probabilistic Robotics}. MIT Press
    \item Simon, D. (2006). \textit{Optimal State Estimation: Kalman, $H_\infty$, and Nonlinear Approaches}. Wiley
    \item Bar-Shalom, Y., Li, X. R., \& Kirubarajan, T. (2001). \textit{Estimation with Applications to Tracking and Navigation}. Wiley
\end{enumerate}

%----------------------------------------------------------------------------------------
%	APPENDIX
%----------------------------------------------------------------------------------------

\appendix
\section{Summary of Calibration Parameters}

\begin{table}[H]
\centering
\caption{Complete Calibration Parameters Summary}
\begin{tabular}{llc}
\toprule
\textbf{Sensor} & \textbf{Parameter} & \textbf{Value} \\
\midrule
\multirow{6}{*}{Gyroscope} & Bias X & $-0.0324$ deg/s \\
 & Bias Y & $-0.2039$ deg/s \\
 & Bias Z & $0.1873$ deg/s \\
 & Variance X & $0.0414$ (deg/s)$^2$ \\
 & Variance Y & $0.0477$ (deg/s)$^2$ \\
 & Variance Z & $0.0756$ (deg/s)$^2$ \\
\midrule
\multirow{6}{*}{Accelerometer} & Gain $k_x$ & $0.9975$ \\
 & Gain $k_y$ & $0.9929$ \\
 & Gain $k_z$ & $0.9961$ \\
 & Bias $b_x$ & $0.0136$ g \\
 & Bias $b_y$ & $-0.0042$ g \\
 & Bias $b_z$ & $0.0193$ g \\
\midrule
\multirow{3}{*}{Camera} & Focal length $f$ & $524.72$ pixels \\
 & Distance bias $b$ & $3.42$ cm \\
 & $R^2$ & $0.9997$ \\
\midrule
\multirow{2}{*}{Motor} & Speed (30\% PWM) & $6.23$ cm/s \\
 & Est. full speed & $20.77$ cm/s \\
\bottomrule
\end{tabular}
\end{table}

\section{Optimized EKF Parameters}

\begin{table}[H]
\centering
\caption{Final Extended Kalman Filter Parameters}
\begin{tabular}{llc}
\toprule
\textbf{Category} & \textbf{Parameter} & \textbf{Value} \\
\midrule
\multirow{3}{*}{State Vector} & Position $p_x$ & cm \\
 & Position $p_y$ & cm \\
 & Heading $\phi$ & radians \\
\midrule
\multirow{2}{*}{Control Input} & Linear velocity $v$ & cm/s \\
 & Angular velocity $\omega$ & rad/s \\
\midrule
\multirow{3}{*}{Process Noise} & $\sigma_v$ & $0.4$ cm/s \\
 & $\sigma_\omega$ & $0.04$ rad/s ($\approx 2.3°$/s) \\
 & Sampling rate & 20 Hz (IMU) \\
\midrule
\multirow{3}{*}{Measurement Noise} & $\sigma_h$ & $2.5$ cm (height) \\
 & $\sigma_{C_x}$ & $3.5°$ (converted to pixels) \\
 & Camera rate & $\sim 5$ Hz \\
\midrule
\multirow{2}{*}{Outlier Rejection} & Chi-square threshold $\gamma$ & 15.0 \\
 & Rejection rate & 60.0\% \\
\midrule
\multirow{3}{*}{Initial Conditions} & $p_x(0)$ & $43$ cm \\
 & $p_y(0)$ & $18$ cm \\
 & $\phi(0)$ & $0$ rad ($0°$) \\
\midrule
\multirow{3}{*}{Initial Covariance} & $P_{xx}(0)$ & $1.0$ cm$^2$ \\
 & $P_{yy}(0)$ & $1.0$ cm$^2$ \\
 & $P_{\phi\phi}(0)$ & $0.01$ rad$^2$ \\
\midrule
\multirow{2}{*}{Velocity Model} & PWM scaling & Nonlinear \\
 & Exponent & 1.15 \\
\bottomrule
\end{tabular}
\end{table}

\section{Particle Filter Parameters}

\begin{table}[H]
\centering
\caption{Particle Filter Configuration (for comparison)}
\begin{tabular}{llc}
\toprule
\textbf{Category} & \textbf{Parameter} & \textbf{Value} \\
\midrule
\multirow{2}{*}{Filter Configuration} & Number of particles $N$ & 2000 \\
 & Resampling threshold & 1000 ($N_{\text{eff}}$) \\
\midrule
\multirow{2}{*}{Process Noise} & $\sigma_v$ & $2.0$ cm/s \\
 & $\sigma_\omega$ & $5.0°$/s \\
\midrule
\multirow{2}{*}{Measurement Noise} & $\sigma_h$ & $2.0$ pixels \\
 & $\sigma_{C_x}$ & $3.0$ pixels \\
\midrule
\multirow{3}{*}{Initial Conditions} & $p_x(0)$ & $43$ cm \\
 & $p_y(0)$ & $18$ cm \\
 & $\phi(0)$ & $0°$ \\
\midrule
\multirow{2}{*}{Performance} & Position drift & 11.4 cm \\
 & Heading error & 2.0° \\
\bottomrule
\end{tabular}
\end{table}

\section{Performance Comparison Summary}

\begin{table}[H]
\centering
\caption{Algorithm Performance Comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Dead Reckoning} & \textbf{Particle Filter} & \textbf{Optimized EKF} \\
\midrule
\multicolumn{4}{c}{\textit{Accuracy}} \\
Position drift (cm) & 5.92 & 11.4 & \textbf{4.9} \\
Drift per lap (cm) & -- & 3.8 & \textbf{1.6} \\
Heading error (deg) & 21.4 & 2.0 & 4.7 \\
Heading drift (deg/s) & 0.10 & 0.011 & \textbf{0.024} \\
\midrule
\multicolumn{4}{c}{\textit{Uncertainty Quantification}} \\
Final $\sigma_x$ (cm) & N/A & -- & \textbf{0.77} \\
Final $\sigma_y$ (cm) & N/A & -- & \textbf{0.34} \\
Final $\sigma_\phi$ (deg) & N/A & -- & \textbf{< 1.0} \\
\midrule
\multicolumn{4}{c}{\textit{Robustness}} \\
Outlier rejection & None & None & \textbf{Yes (60\%)} \\
Measurements used & 100\% & 100\% & \textbf{40\%} \\
Loop closure error & 5.92 & 11.4 cm & \textbf{4.9 cm} \\
\midrule
\multicolumn{4}{c}{\textit{Computational Aspects}} \\
Update rate & 20 Hz & 20 Hz & 20 Hz \\
Complexity & Low & High & Medium \\
Tuning difficulty & N/A & Medium & High but rewarding \\
\midrule
\textbf{Overall Ranking} & \textbf{3rd} & \textbf{2nd} & \textbf{1st} \\
\bottomrule
\end{tabular}
\end{table}

\section{Key Tuning Insights}

\subsection{Process Noise Selection}

The process noise covariance $\bm{Q}$ represents uncertainty in the motion model. For the EKF:

\begin{equation}
    \bm{Q}_k = \begin{bmatrix}
        \sigma_v^2 \cos^2(\phi) \Delta t^2 & \sigma_v^2 \cos(\phi)\sin(\phi) \Delta t^2 & 0 \\
        \sigma_v^2 \cos(\phi)\sin(\phi) \Delta t^2 & \sigma_v^2 \sin^2(\phi) \Delta t^2 & 0 \\
        0 & 0 & \sigma_\omega^2 \Delta t^2
    \end{bmatrix}
\end{equation}

\textbf{Selection rationale:}
\begin{itemize}
    \item $\sigma_v = 0.4$ cm/s: Represents typical velocity uncertainty due to PWM variations, wheel slip, and modeling errors. Much smaller than PF value (2.0 cm/s) because EKF can be more confident with outlier rejection.
    
    \item $\sigma_\omega = 0.04$ rad/s: Accounts for gyroscope noise and integration errors. Smaller than PF value (0.087 rad/s) due to better gyroscope calibration and outlier rejection.
    
    \item Key insight: \textbf{Smaller process noise} works better with aggressive outlier rejection because the filter trusts the model more and only incorporates high-quality measurements.
\end{itemize}

\subsection{Measurement Noise Selection}

The measurement noise covariance $\bm{R}$ represents camera measurement uncertainty:

\begin{equation}
    \bm{R} = \text{diag}(\sigma_h^2, \sigma_{C_x}^2, \sigma_h^2, \sigma_{C_x}^2, \ldots)
\end{equation}

\textbf{Selection rationale:}
\begin{itemize}
    \item $\sigma_h = 2.5$ cm: Height measurement uncertainty. Larger than PF (2.0 pixels $\approx 1$ cm) to account for poor detection quality at various distances.
    
    \item $\sigma_{C_x} = 3.5°$: Angular measurement uncertainty. Converted to pixels as $\sigma_{C_x,\text{px}} = f \tan(\sigma_{C_x}) \approx 32$ pixels for small angles.
    
    \item Key insight: \textbf{Larger measurement noise} combined with outlier rejection allows the filter to accept some uncertainty while still rejecting clear outliers. This is more realistic than overly optimistic noise parameters.
\end{itemize}

\subsection{Outlier Rejection Threshold}

The chi-square threshold $\gamma = 15.0$ determines when measurements are rejected:

\begin{equation}
    d^2 = (\bm{y}_k - g(\hat{\bm{x}}_k^-))^T (\bm{H}_k \bm{P}_k^- \bm{H}_k^T + \bm{R}_k)^{-1} (\bm{y}_k - g(\hat{\bm{x}}_k^-))
\end{equation}

Reject if $d^2 > \gamma$.

\textbf{Selection rationale:}
\begin{itemize}
    \item For 2 degrees of freedom (height + angle per QR code), theoretical thresholds:
    \begin{itemize}
        \item 95\% confidence: $\gamma = 5.99$
        \item 99\% confidence: $\gamma = 9.21$
        \item 99.9\% confidence: $\gamma = 13.82$
    \end{itemize}
    
    \item $\gamma = 15.0$ is slightly above 99.9\% threshold, allowing for some model mismatch
    
    \item Resulted in 60\% rejection rate, indicating camera measurements were indeed noisy
    
    \item Key insight: \textbf{Aggressive outlier rejection} was critical for performance—without it, drift increased to $>15$ cm
\end{itemize}

\subsection{Tuning Procedure}

The optimal parameters were found through systematic experimentation:

\begin{enumerate}
    \item \textbf{Baseline}: Started with theoretically motivated values from sensor calibration
    
    \item \textbf{Process noise}: Reduced iteratively while monitoring innovation statistics until filter became too confident
    
    \item \textbf{Measurement noise}: Increased to realistic levels based on residual analysis
    
    \item \textbf{Outlier threshold}: Tuned to reject clearly bad measurements while accepting marginal ones
    
    \item \textbf{Validation}: Verified on multiple runs that performance was consistent and not due to lucky parameter selection
\end{enumerate}

\end{document}